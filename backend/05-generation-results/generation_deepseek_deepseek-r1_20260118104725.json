{
  "query": "What is GRPO",
  "timestamp": "2026-01-18T10:47:25.198553",
  "provider": "deepseek",
  "model": "deepseek-r1",
  "response": "ã€æ€ç»´è¿‡ç¨‹ã€‘\nOkay, let me start by reading the user's question and the provided context. The question is: \"What is GRPO?\" The context is from Source 1, which talks about Group Relative Policy Optimization (GRPO).\n\nReading through the context, it mentions that GRPO is used to save training costs in RL (Reinforcement Learning). It foregoes the critic model, which is usually the same size as the policy model. Instead, it estimates the baseline from group scores. The method samples a group of outputs from the old policy for each question and optimizes the policy model by maximizing a specific objective.\n\nHmm, so GRPO is an optimization method in reinforcement learning. The key points here are that it avoids using a separate critic model, which reduces costs, and uses group scores for baseline estimation. I should summarize this clearly, mentioning its purpose (cost-saving in RL training), how it works (sampling outputs from old policy, using group scores instead of a critic), and maybe the reference (Shao et al., 2024).\n\nã€æœ€ç»ˆç­”æ¡ˆã€‘\nBased on the provided context:\n\nGRPO stands for **Group Relative Policy Optimization**, a reinforcement learning (RL) method designed to reduce training costs. It eliminates the need for a separate critic model (which is typically as large as the policy model) by estimating the baseline from the scores of a group of sampled outputs instead.",
  "context": [
    {
      "text": "Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question ğ‘, GRPO samples a group of outputs {ğ‘œ1, ğ‘œ2, Â· Â· Â· , ğ‘œğº} from the old policy ğœ‹ğœƒğ‘œğ‘™ğ‘‘ and then optimizes the policy model ğœ‹ğœƒ by maximizing the following objective:",
      "score": 0.5254793167114258,
      "metadata": {
        "source": "DeepSeek_R1.pdf",
        "page": "5",
        "chunk": 41,
        "total_chunks": 180,
        "page_range": "5",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-02-14T16:48:03.946618"
      }
    }
  ]
}